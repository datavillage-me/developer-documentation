{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Datavillage Documentation Learn how to use Datavillage to combine data with partners and apply AI algorithms without disclosing sensitive information. Getting started with Datavillage Learn about Datavillage, setup and run a basic data collaboration, explore features and use cases. How can Datavillage help you ? Keep control over data Control who and how data is used within every collaboration. Access more data Access partner data and run AI algorithms on combined datasets. Protect confidentiality Prevent disclosure of proprietary, sensitive or personal information with confidential Datacages. Ensure transparency Enable algorithm transparency and explainability with algorithm descriptors and our lightweight SDK. Get data and algorithms into Datavillage ? The data collaboration platform helps you define and connect data with algorithms between partners through APIs as well as downstream tools. Console Manage collaboration governance through an intuitive interface. API Integrate all features via a complete set of open API. CLI Create and run scripts with our command-line interface. Want more hands-on guidance? Book a Demo and get 2 hours of data valorization for free.","title":"Home"},{"location":"index.html#datavillage-documentation","text":"Learn how to use Datavillage to combine data with partners and apply AI algorithms without disclosing sensitive information. Getting started with Datavillage Learn about Datavillage, setup and run a basic data collaboration, explore features and use cases.","title":"Datavillage Documentation"},{"location":"index.html#how-can-datavillage-help-you","text":"Keep control over data Control who and how data is used within every collaboration. Access more data Access partner data and run AI algorithms on combined datasets. Protect confidentiality Prevent disclosure of proprietary, sensitive or personal information with confidential Datacages. Ensure transparency Enable algorithm transparency and explainability with algorithm descriptors and our lightweight SDK.","title":"How can Datavillage help you ?"},{"location":"index.html#get-data-and-algorithms-into-datavillage","text":"The data collaboration platform helps you define and connect data with algorithms between partners through APIs as well as downstream tools. Console Manage collaboration governance through an intuitive interface. API Integrate all features via a complete set of open API. CLI Create and run scripts with our command-line interface.","title":"Get data and algorithms into Datavillage ?"},{"location":"index.html#want-more-hands-on-guidance","text":"Book a Demo and get 2 hours of data valorization for free.","title":"Want more hands-on guidance?"},{"location":"10-getting-started.html","text":"Getting started Register your organization 1. Go to the collaboration console and register your organization. 2. You will receive an email to confirm your registration and log in as the first employee - owner of the collaboration space. Create a new collaboration space Click the New Collaboration Space button present on the home page. Enter the your collaboration space preferences. Confirm the creation. Invite participants 1. Click on the Add more button for data providers to ask participants to join the collaboration space. Add yourself if your organization is itself a data provider. 2. Click on the Add more button for code provider to ask the participant to join the collaboration space. Add yourself if your organization is itself a code provider (only one code provider per collaboration space). 3. Click on the Add more button for data consumers to ask participants to join the collaboration space. Add yourself if your organization is itself a data consumer. Next steps Go to the Console menu to see how the Collaboration Console helps you describe, manage, and perform your collaboration as a confidential data product.","title":"Getting started"},{"location":"10-getting-started.html#getting-started","text":"","title":"Getting started"},{"location":"10-getting-started.html#register-your-organization","text":"1. Go to the collaboration console and register your organization. 2. You will receive an email to confirm your registration and log in as the first employee - owner of the collaboration space.","title":"Register your organization"},{"location":"10-getting-started.html#create-a-new-collaboration-space","text":"Click the New Collaboration Space button present on the home page. Enter the your collaboration space preferences. Confirm the creation.","title":"Create a new collaboration space"},{"location":"10-getting-started.html#invite-participants","text":"1. Click on the Add more button for data providers to ask participants to join the collaboration space. Add yourself if your organization is itself a data provider. 2. Click on the Add more button for code provider to ask the participant to join the collaboration space. Add yourself if your organization is itself a code provider (only one code provider per collaboration space). 3. Click on the Add more button for data consumers to ask participants to join the collaboration space. Add yourself if your organization is itself a data consumer.","title":"Invite participants"},{"location":"10-getting-started.html#next-steps","text":"Go to the Console menu to see how the Collaboration Console helps you describe, manage, and perform your collaboration as a confidential data product.","title":"Next steps"},{"location":"90-sdk.html","text":"Datavillage enables organizations to collaborate on data with other organizations and their consumers with privacy. Organizations gain access to sensitive and personal data of other parties while confidentiality and integrity of data and algorithms are guaranteed. Organizations and consumers can share data without fear of it being misused or of losing their competitive advantage. It\u2019s about sharing without showing . Datavillage provides the D ata C ollaboration P latform implementing privacy by design and automating trusted and transparent insights generation. Privacy by design is implemented based on end-to-end data encryption and transparent governance: - Data are encrypted at rest and in transit - Data are encrypted while beeing processed - Confidential algorithm is running is a fully sandboxed environment - Only derived data can be accessed by data consumer - Consumers are in control of their data and can join collaborations with explicit consent Data collaboration spaces Organizations can create data collaboration spaces involving other organizations and their consumers directly. The participants meet in the confidential environment: - The data providers that provide the raw data. The data providers can be organizations providing enterprise data or consumers directly connecting zero-party personal data through their data vault. The data is only visible in the secure environment by the trusted algorithm. - The code provider that provides the algorithm that will run on the data. The code provider describes the algorithm and the output model (ontology) for derived data. - The data consumers who access the results of the algorithm (derived data). Only data consumers can access the result from the confidential environment. Consumers get result directly in their personal data vault. As illustrated in the diagram below, a participant can have several roles. End-to-end confidentiality End-to-end confidentiality guarantees each participant a complete level of control and transparency over the collaboration: - The identity of each participant both organizations and consumers is known and validated - The data can only be accessed in the confidential environment by the approved code. Personal data are stored in the data vault of consumers. - Only the derived data (results) can leave the confidential environment by the authorized participant - The purpose of the collaboration is validated by each participant including consumers via their explicit consent - The attestation that the confidential environment is indeed a trusted environment and complies with the requirements - The confidential environment, also called Datacage , operates on TEE enclaves, sanboxed via micro-firewalls. Participants SDK Software development kit and code snippets for participants of a collaboration space. Data provider SDK There are two different ways to connect data with the Datacage: - Push approach where the data provider will push the data, encrypted beforehand with the public key received during the verification of the attestation (a flow based on the exchange of symmetric keys is also possible for large datasets). The push model is recommended for datasets that do not require frequent synchronization . Pull approach where the Data Provider will pull the data from the Datacage. The data can be pre-encrypted on the data provider side, but we recommend transport layer security where the mutual authentication certificate is created and verified based on the enclave key. The pull model is recommended for datasets that do require more frequent synchronization . These are the Data provider connectors currently supported by the platform: Enterprise data systems Connect enterprise data from amazon S3 buckets (Push) Connect enterprise data from Azure blob storage (Push) Personal data systems (data vaults) Connect directly Enterprise Solid Server (Inrupt) from consumers as data provider (Pull) Connect directly Solid Community Server from consumers as data provider (Pull) Code provider SDK The Python SDK allows the code provider to quickly write an algorithm that can work and access the functionality of the Datacage . Development languages and tools Use Python as development language for the algorithm Use Notebook to build your algorithm (on sample datasets) Data consumer SDK The data consumer SDK is under construction.","title":"Participants SDK"},{"location":"90-sdk.html#data-collaboration-spaces","text":"Organizations can create data collaboration spaces involving other organizations and their consumers directly. The participants meet in the confidential environment: - The data providers that provide the raw data. The data providers can be organizations providing enterprise data or consumers directly connecting zero-party personal data through their data vault. The data is only visible in the secure environment by the trusted algorithm. - The code provider that provides the algorithm that will run on the data. The code provider describes the algorithm and the output model (ontology) for derived data. - The data consumers who access the results of the algorithm (derived data). Only data consumers can access the result from the confidential environment. Consumers get result directly in their personal data vault. As illustrated in the diagram below, a participant can have several roles.","title":"Data collaboration spaces"},{"location":"90-sdk.html#end-to-end-confidentiality","text":"End-to-end confidentiality guarantees each participant a complete level of control and transparency over the collaboration: - The identity of each participant both organizations and consumers is known and validated - The data can only be accessed in the confidential environment by the approved code. Personal data are stored in the data vault of consumers. - Only the derived data (results) can leave the confidential environment by the authorized participant - The purpose of the collaboration is validated by each participant including consumers via their explicit consent - The attestation that the confidential environment is indeed a trusted environment and complies with the requirements - The confidential environment, also called Datacage , operates on TEE enclaves, sanboxed via micro-firewalls.","title":"End-to-end confidentiality"},{"location":"90-sdk.html#participants-sdk","text":"Software development kit and code snippets for participants of a collaboration space.","title":"Participants SDK"},{"location":"90-sdk.html#data-provider-sdk","text":"There are two different ways to connect data with the Datacage: - Push approach where the data provider will push the data, encrypted beforehand with the public key received during the verification of the attestation (a flow based on the exchange of symmetric keys is also possible for large datasets). The push model is recommended for datasets that do not require frequent synchronization . Pull approach where the Data Provider will pull the data from the Datacage. The data can be pre-encrypted on the data provider side, but we recommend transport layer security where the mutual authentication certificate is created and verified based on the enclave key. The pull model is recommended for datasets that do require more frequent synchronization . These are the Data provider connectors currently supported by the platform:","title":"Data provider SDK"},{"location":"90-sdk.html#enterprise-data-systems","text":"Connect enterprise data from amazon S3 buckets (Push) Connect enterprise data from Azure blob storage (Push)","title":"Enterprise data systems"},{"location":"90-sdk.html#personal-data-systems-data-vaults","text":"Connect directly Enterprise Solid Server (Inrupt) from consumers as data provider (Pull) Connect directly Solid Community Server from consumers as data provider (Pull)","title":"Personal data systems (data vaults)"},{"location":"90-sdk.html#code-provider-sdk","text":"The Python SDK allows the code provider to quickly write an algorithm that can work and access the functionality of the Datacage .","title":"Code provider SDK"},{"location":"90-sdk.html#development-languages-and-tools","text":"Use Python as development language for the algorithm Use Notebook to build your algorithm (on sample datasets)","title":"Development languages and tools"},{"location":"90-sdk.html#data-consumer-sdk","text":"The data consumer SDK is under construction.","title":"Data consumer SDK"},{"location":"20-getting-started/01-what-is-datavillage.html","text":"What is Datavillage? With Datavillage, you can combine personal, sensitive or proprietary data between partners and apply AI algorithms without disclosing any information. Datavillage simplifies the governance of data collaboration and algorithm execution in completely confidential environments, allowing participants to spend more time getting the most out of their data. You can use Datavillage to enable Secure Data Sharing by aggregating and correlating datasets, apply anonymization, pseudonymization or minimization in confidential environments to train and run AI algorithms without losing control and confidentiality of your data and models - also called Confidential AI to set up secure multi-party data collaborations in a transparent governance and perform Collaborative Analytics to provide Personalisation with Privacy and process personal data of 3rd parties combined with yours in a privacy-preserving way When you collaborate on Datavillage, you can integrate your data governance, connect all data sources, publish and run algorithms with your favorite tools to gain new insights (derived data) with confidentiality, transparency, and trust but always verify.","title":"What is Datavillage?"},{"location":"20-getting-started/01-what-is-datavillage.html#what-is-datavillage","text":"With Datavillage, you can combine personal, sensitive or proprietary data between partners and apply AI algorithms without disclosing any information. Datavillage simplifies the governance of data collaboration and algorithm execution in completely confidential environments, allowing participants to spend more time getting the most out of their data. You can use Datavillage to enable Secure Data Sharing by aggregating and correlating datasets, apply anonymization, pseudonymization or minimization in confidential environments to train and run AI algorithms without losing control and confidentiality of your data and models - also called Confidential AI to set up secure multi-party data collaborations in a transparent governance and perform Collaborative Analytics to provide Personalisation with Privacy and process personal data of 3rd parties combined with yours in a privacy-preserving way When you collaborate on Datavillage, you can integrate your data governance, connect all data sources, publish and run algorithms with your favorite tools to gain new insights (derived data) with confidentiality, transparency, and trust but always verify.","title":"What is Datavillage?"},{"location":"20-getting-started/02-how-it-works.html","text":"How Datavillage Works In a nutshell, the collaboration owner creates a collaboration space and invites participants as data providers, algorithm providers, or insights consumers. Multiple roles can be assumed by a single participant. Data providers connect their encrypted data via custom APIs or available data connectors, algorithm providers publish their algorithm via Dockerfile and data consumers consume the result via custom APIs, webhooks or data connectors. The Datacage associated with the collaboration space handles the events (batch or real time), executes the algorithm on the decrypted data (only the algorithm can access the data in clear) and the result is made available to insightsx consumers in an encrypted way. The Datacage verifies authorization, consent (if required from individuals) and keeps track of all data activity in an audit trail. Overview Data Providers are the organizations that link their data to the collaboration space and Data Connectors are how they can connect their data in a fully encrypted way. Code Providers or Algorithm Providers are the organizations that publish their algorithm in the collaboration and Algorithm Models are examples of typical algorithms (rule-based, AI- and ML-based or knowledge graphs based) running in the confidential environment, being able to handle the events sent to the collaboration space and to execute to provide an encrypted result on the combined data of the data providers. Data consumers are the organizations that get access to the decrypted result of the confidential treatment and can use this result with the guarantee that it has been calculated with the agreement of the participants and in compliance with the regulations. Sources for connecting data You can connect all types of data such as personal data, sensitive data or proprietary data with the guarantee that the data will never be decrypted except by the trusted algorithm associated with the collaboration space. Connecting Enterprise Data Enterprise data connectors are Datavillage's powerful way to connect large data sets from any data provider on any cloud provider without the need to copy and transfer all the data into the collaboration space. Fresh data is directly accessible and used. With end-to-end encryption as well as \"in-use\" encryption, data is processed in encrypted memory ensuring privacy and control. Connecting Personal Data Personal data connectors are the way Datavillage connects personal data while ensuring control and privacy for individuals. Personal data is accessible through a granular consent mechanism and are processed in encrypted memory. Connecting with custom APIs Custom APIs can be used to push and pull data. The mechanism is mainly used for data providers relying on legacy systems. Data are transfered into the collaboration space and remain encrypted at rest an in use. How you can process data Building and publishing algorithm Building and publishing an algorithm is very simple. To build your algorithm you can base yourself on available models or start from scratch. You publish your algorithm through standard Docker containers that you connect through your Docker registry to the collaboration space. The supported language today is Python Executing algorithm The algorithm can be executed in batch or in (near) real time. Execution can be triggered through events sent to the collaboration space and processed by the algorithm in the confidential environment. Enabling transparency and explainability An algorithm descriptor is required with the algorithm published in the collaboration space. It describes its purpose and how it works. Explainability can be handled through encrypted outputs to result consumers. Where you can share results The results can be shared in an encrypted way towards the result consumers either through information accessible in the collaboration space or through webhook exposed by the result consumer. The result must correspond to the structure, format and types of data defined and approved by the collaboration space.","title":"How Datavillage Works"},{"location":"20-getting-started/02-how-it-works.html#how-datavillage-works","text":"In a nutshell, the collaboration owner creates a collaboration space and invites participants as data providers, algorithm providers, or insights consumers. Multiple roles can be assumed by a single participant. Data providers connect their encrypted data via custom APIs or available data connectors, algorithm providers publish their algorithm via Dockerfile and data consumers consume the result via custom APIs, webhooks or data connectors. The Datacage associated with the collaboration space handles the events (batch or real time), executes the algorithm on the decrypted data (only the algorithm can access the data in clear) and the result is made available to insightsx consumers in an encrypted way. The Datacage verifies authorization, consent (if required from individuals) and keeps track of all data activity in an audit trail.","title":"How Datavillage Works"},{"location":"20-getting-started/02-how-it-works.html#overview","text":"Data Providers are the organizations that link their data to the collaboration space and Data Connectors are how they can connect their data in a fully encrypted way. Code Providers or Algorithm Providers are the organizations that publish their algorithm in the collaboration and Algorithm Models are examples of typical algorithms (rule-based, AI- and ML-based or knowledge graphs based) running in the confidential environment, being able to handle the events sent to the collaboration space and to execute to provide an encrypted result on the combined data of the data providers. Data consumers are the organizations that get access to the decrypted result of the confidential treatment and can use this result with the guarantee that it has been calculated with the agreement of the participants and in compliance with the regulations.","title":"Overview"},{"location":"20-getting-started/02-how-it-works.html#sources-for-connecting-data","text":"You can connect all types of data such as personal data, sensitive data or proprietary data with the guarantee that the data will never be decrypted except by the trusted algorithm associated with the collaboration space.","title":"Sources for connecting data"},{"location":"20-getting-started/02-how-it-works.html#connecting-enterprise-data","text":"Enterprise data connectors are Datavillage's powerful way to connect large data sets from any data provider on any cloud provider without the need to copy and transfer all the data into the collaboration space. Fresh data is directly accessible and used. With end-to-end encryption as well as \"in-use\" encryption, data is processed in encrypted memory ensuring privacy and control.","title":"Connecting Enterprise Data"},{"location":"20-getting-started/02-how-it-works.html#connecting-personal-data","text":"Personal data connectors are the way Datavillage connects personal data while ensuring control and privacy for individuals. Personal data is accessible through a granular consent mechanism and are processed in encrypted memory.","title":"Connecting Personal Data"},{"location":"20-getting-started/02-how-it-works.html#connecting-with-custom-apis","text":"Custom APIs can be used to push and pull data. The mechanism is mainly used for data providers relying on legacy systems. Data are transfered into the collaboration space and remain encrypted at rest an in use.","title":"Connecting with custom APIs"},{"location":"20-getting-started/02-how-it-works.html#how-you-can-process-data","text":"","title":"How you can process data"},{"location":"20-getting-started/02-how-it-works.html#building-and-publishing-algorithm","text":"Building and publishing an algorithm is very simple. To build your algorithm you can base yourself on available models or start from scratch. You publish your algorithm through standard Docker containers that you connect through your Docker registry to the collaboration space. The supported language today is Python","title":"Building and publishing algorithm"},{"location":"20-getting-started/02-how-it-works.html#executing-algorithm","text":"The algorithm can be executed in batch or in (near) real time. Execution can be triggered through events sent to the collaboration space and processed by the algorithm in the confidential environment.","title":"Executing algorithm"},{"location":"20-getting-started/02-how-it-works.html#enabling-transparency-and-explainability","text":"An algorithm descriptor is required with the algorithm published in the collaboration space. It describes its purpose and how it works. Explainability can be handled through encrypted outputs to result consumers.","title":"Enabling transparency and explainability"},{"location":"20-getting-started/02-how-it-works.html#where-you-can-share-results","text":"The results can be shared in an encrypted way towards the result consumers either through information accessible in the collaboration space or through webhook exposed by the result consumer. The result must correspond to the structure, format and types of data defined and approved by the collaboration space.","title":"Where you can share results"},{"location":"Howto/10-using-platform.html","text":"Using the platform The Datavillage platform is accessible through several channels : - the web UI - the Command Line Interface - the Typescript client - the HTTP API Web UI Obtaining an API token Command Line Interface Installation The Datavillage Platform CLI can be installed from the public NPM registry, resulting in a global datavillage executable : $> npm i @datavillage-me/platform-cli -g $> datavillage --help It can also be directly executed with npx : $> npx @datavillage-me/platform-cli Authentication The login command will open a browser window and redirect to the platform login page. On success, the resulting token and the server URL will be stored in $HOME/.datavillage/config.json # The default login flow uses the Dev login page (github) $> datavillage auth login # Force login as a user (using a pod provider) $> datavillage auth login --user # Force login to a specific server instance $> datavillage auth login --url = https://api-dev.datavillage.me # Display the current credentials $> datavillage auth whoami Typescript API client The DataVillage Platform API provides an abstract access to all the functionalities exposed by the DataVillage platform. The Typescript API Client is an HTTP client that leverages the HTTP API (below). It can be used in a Node environment or a browser. Instantiating the API client The main method to obtain a client is getRemoteClient : import { getRemoteClient } from '@datavillage-me/api' ; const client = getRemoteClient ( 'https://api-dev.datavillage.me' ); The client can then be used to get proxies to the different subcomponents of the platform : const consentManager = client . getConsentManager (); client . getConsentReceipts (). then (( receipts ) => {...}); Authentication Providing the auth token (Node) The remote client requires some valid authentication token to perform requests. In a Node environment, such an authentication token can be passed explicitly in the constructor method : const authToken = ...; // auth token obtained previously const client = getRemoteClient ( 'https://api-dev.datavillage.me' , authToken ); Triggering the login flow in a browser In a the browser environment, assuming the user has authenticated to the backend previously, the authentication token present in the browser cookies will automatically be appended to outgoing requests. In that case, the call to getRemoteClient can omit the auth token. To obtain an authentication token as a browser cookie, the login flow has to be performed. This can be done by redirecting the user to the URL provided by Client.getPassport().getAuthenticationUrl() . Building that URL requires the pod type ('solid' or 'google'), the redirectUrl (where to go after the login succeeds), and optionally the identity issuer (for Solid, this can be f.i. 'http://inrupt.net', 'https://solidcommunity.net'). const client = getRemoteClient ( 'https://api-dev.datavillage.me' ); // let's return to this same page after login const redirectUrl = window . location . href ; // if issuer is not provided for Solid, a default one will be used const authUrl = await client . getPassport (). getAuthenticationUrl ( POD_IDENTITY_PROVIDER . Solid , redirectUrl ); window . location . href = authUrl ; HTTP API All of the three previous methods actually rely on an HTTP API. That API can also be used directly. Authentication","title":"Using the platform"},{"location":"Howto/10-using-platform.html#using-the-platform","text":"The Datavillage platform is accessible through several channels : - the web UI - the Command Line Interface - the Typescript client - the HTTP API","title":"Using the platform"},{"location":"Howto/10-using-platform.html#web-ui","text":"","title":"Web UI"},{"location":"Howto/10-using-platform.html#obtaining-an-api-token","text":"","title":"Obtaining an API token"},{"location":"Howto/10-using-platform.html#command-line-interface","text":"","title":"Command Line Interface"},{"location":"Howto/10-using-platform.html#installation","text":"The Datavillage Platform CLI can be installed from the public NPM registry, resulting in a global datavillage executable : $> npm i @datavillage-me/platform-cli -g $> datavillage --help It can also be directly executed with npx : $> npx @datavillage-me/platform-cli","title":"Installation"},{"location":"Howto/10-using-platform.html#authentication","text":"The login command will open a browser window and redirect to the platform login page. On success, the resulting token and the server URL will be stored in $HOME/.datavillage/config.json # The default login flow uses the Dev login page (github) $> datavillage auth login # Force login as a user (using a pod provider) $> datavillage auth login --user # Force login to a specific server instance $> datavillage auth login --url = https://api-dev.datavillage.me # Display the current credentials $> datavillage auth whoami","title":"Authentication"},{"location":"Howto/10-using-platform.html#typescript-api-client","text":"The DataVillage Platform API provides an abstract access to all the functionalities exposed by the DataVillage platform. The Typescript API Client is an HTTP client that leverages the HTTP API (below). It can be used in a Node environment or a browser.","title":"Typescript API client"},{"location":"Howto/10-using-platform.html#instantiating-the-api-client","text":"The main method to obtain a client is getRemoteClient : import { getRemoteClient } from '@datavillage-me/api' ; const client = getRemoteClient ( 'https://api-dev.datavillage.me' ); The client can then be used to get proxies to the different subcomponents of the platform : const consentManager = client . getConsentManager (); client . getConsentReceipts (). then (( receipts ) => {...});","title":"Instantiating the API client"},{"location":"Howto/10-using-platform.html#authentication_1","text":"","title":"Authentication"},{"location":"Howto/10-using-platform.html#providing-the-auth-token-node","text":"The remote client requires some valid authentication token to perform requests. In a Node environment, such an authentication token can be passed explicitly in the constructor method : const authToken = ...; // auth token obtained previously const client = getRemoteClient ( 'https://api-dev.datavillage.me' , authToken );","title":"Providing the auth token (Node)"},{"location":"Howto/10-using-platform.html#triggering-the-login-flow-in-a-browser","text":"In a the browser environment, assuming the user has authenticated to the backend previously, the authentication token present in the browser cookies will automatically be appended to outgoing requests. In that case, the call to getRemoteClient can omit the auth token. To obtain an authentication token as a browser cookie, the login flow has to be performed. This can be done by redirecting the user to the URL provided by Client.getPassport().getAuthenticationUrl() . Building that URL requires the pod type ('solid' or 'google'), the redirectUrl (where to go after the login succeeds), and optionally the identity issuer (for Solid, this can be f.i. 'http://inrupt.net', 'https://solidcommunity.net'). const client = getRemoteClient ( 'https://api-dev.datavillage.me' ); // let's return to this same page after login const redirectUrl = window . location . href ; // if issuer is not provided for Solid, a default one will be used const authUrl = await client . getPassport (). getAuthenticationUrl ( POD_IDENTITY_PROVIDER . Solid , redirectUrl ); window . location . href = authUrl ;","title":"Triggering the login flow in a browser"},{"location":"Howto/10-using-platform.html#http-api","text":"All of the three previous methods actually rely on an HTTP API. That API can also be used directly.","title":"HTTP API"},{"location":"Howto/10-using-platform.html#authentication_2","text":"","title":"Authentication"},{"location":"Howto/20-creating-space.html","text":"Setting up a Collaboration Space Create a Collaboration Space This requires MGR privileges for client used as owner. Console HTTP CLI TypeScript # the Client ID of the Owner must be provided $> datavillage space create MyNewSpace 63ff2995c37e866eff4b8634 # Get the list of collaboration spaces that the current user has access to $> datavillage space list import { getRemoteClient } from '@datavillage-me/api' ; const client = getRemoteClient ( 'https://api-dev.datavillage.me' ); const spaceServices = client . getCollaborationSpacesServices (); let space = await spaceServices . createCollaborationSpace ( { owner : testClient.id , name : \"Test Space\" , category : \"test\" , description : \"A test collaboration space\" , sandboxing : { enabled : false , allowed : [] }, enclaving : { enabled : false } }); Invite Collaborators Console HTTP CLI TypeScript # The collaboration space ID must be provided $> datavillage space add 63ff30d9d2b0b893c4b5868a # set the algo settings $> datavillage space code set 63ff30d9d2b0b893c4b5868a","title":"Set up a Collaboration Space"},{"location":"Howto/20-creating-space.html#setting-up-a-collaboration-space","text":"","title":"Setting up a Collaboration Space"},{"location":"Howto/20-creating-space.html#create-a-collaboration-space","text":"This requires MGR privileges for client used as owner. Console HTTP CLI TypeScript # the Client ID of the Owner must be provided $> datavillage space create MyNewSpace 63ff2995c37e866eff4b8634 # Get the list of collaboration spaces that the current user has access to $> datavillage space list import { getRemoteClient } from '@datavillage-me/api' ; const client = getRemoteClient ( 'https://api-dev.datavillage.me' ); const spaceServices = client . getCollaborationSpacesServices (); let space = await spaceServices . createCollaborationSpace ( { owner : testClient.id , name : \"Test Space\" , category : \"test\" , description : \"A test collaboration space\" , sandboxing : { enabled : false , allowed : [] }, enclaving : { enabled : false } });","title":"Create a Collaboration Space"},{"location":"Howto/20-creating-space.html#invite-collaborators","text":"Console HTTP CLI TypeScript # The collaboration space ID must be provided $> datavillage space add 63ff30d9d2b0b893c4b5868a # set the algo settings $> datavillage space code set 63ff30d9d2b0b893c4b5868a","title":"Invite Collaborators"},{"location":"Howto/30-add-input-data.html","text":"Adding Input Datasets Configure an input dataset Console HTTP CLI TypeScript","title":"Adding Input Datasets"},{"location":"Howto/30-add-input-data.html#adding-input-datasets","text":"","title":"Adding Input Datasets"},{"location":"Howto/30-add-input-data.html#configure-an-input-dataset","text":"Console HTTP CLI TypeScript","title":"Configure an input dataset"},{"location":"Howto/40-configure-algorithm.html","text":"Configure Algorithm Console HTTP CLI TypeScript","title":"Configure Algorithm"},{"location":"Howto/40-configure-algorithm.html#configure-algorithm","text":"Console HTTP CLI TypeScript","title":"Configure Algorithm"},{"location":"api/api.html","text":"SwaggerUI window.onload = () => { window.ui = SwaggerUIBundle({ url: '../generated/openapi.yaml', dom_id: '#swagger-ui', }); };","title":"API reference"},{"location":"console/console.html","text":"Collaboration console (old version) You can find everything you need about configuring your collaboration space with the collaboration console in the following sections.","title":"Console (old version)"},{"location":"console/console.html#collaboration-console-old-version","text":"You can find everything you need about configuring your collaboration space with the collaboration console in the following sections.","title":"Collaboration console (old version)"},{"location":"console/datapolicy.html","text":"Data Policy {: .no_toc } The data policy section allows the edition of POD access requested to user when they sign in to the application. Purpose Free text field where you can describe the prupose of the application and why the requested personal data are necessary for the application. This will be directly exposed to the users. Personal data scope This specifies exhaustively the data that the application will access in the user POD. The Data Type represents the type of personal data and the Data Source is the provider of the data. Here is a list actually supported data sources with their types. Data source Data source identifier Available data types Spotify dv:spotify schema:LikeAction schema:DislikeAction schema:FollowAction schema:BookmarkAction schema:ListenAction Facebook dv:facebook schema:LikeAction RTBF Auvio dv:auvio schema:LikeAction schema:DislikeAction VRT dv:vrt schema:LikeAction schema:DislikeAction Processing Scope Application Data Policies are stored gConsent ontology instances. This ontology defines GDPR consent scopes, the relevant scopes have to be selected in the dropdown list.","title":"Data Policy"},{"location":"console/datapolicy.html#data-policy","text":"{: .no_toc } The data policy section allows the edition of POD access requested to user when they sign in to the application.","title":"Data Policy"},{"location":"console/datapolicy.html#purpose","text":"Free text field where you can describe the prupose of the application and why the requested personal data are necessary for the application. This will be directly exposed to the users.","title":"Purpose"},{"location":"console/datapolicy.html#personal-data-scope","text":"This specifies exhaustively the data that the application will access in the user POD. The Data Type represents the type of personal data and the Data Source is the provider of the data. Here is a list actually supported data sources with their types. Data source Data source identifier Available data types Spotify dv:spotify schema:LikeAction schema:DislikeAction schema:FollowAction schema:BookmarkAction schema:ListenAction Facebook dv:facebook schema:LikeAction RTBF Auvio dv:auvio schema:LikeAction schema:DislikeAction VRT dv:vrt schema:LikeAction schema:DislikeAction","title":"Personal data scope"},{"location":"console/datapolicy.html#processing-scope","text":"Application Data Policies are stored gConsent ontology instances. This ontology defines GDPR consent scopes, the relevant scopes have to be selected in the dropdown list.","title":"Processing Scope"},{"location":"console/deploy.html","text":"Deploy {: .no_toc } Connect the trusted code to be deployed in the Datacage . Set up your own algorithm Go to template Datavillage repository and fork it. If the branch to deploy isn't the default repository branch, you can set it in the Branch name field. Click Save to save your repository and the branch to deploy. Write your own algorithm using the provided utility python library to interact with the Datavillage services. This library allows your algorithm to access needed inforamtion such as: |:---------------| | Getting list of registered users | | Getting registered user data | | Getting application settings | | Saving algorithm result to user's pods | | Saving algorithm explains to user's pods | If you don't want to use the library, you can also make the http calls inside the algorithm. A OpenAPI documentation is available ths API section Enter the clone SSH git url of your repository. For GitHub you can find this URL here: Click the Generate deploy key , this will generate a public key and display it in the Deploy key text area. Copy this key and save it to your repository settings. For GitHub, you can follow these steps and skip the first 3 as Datavillage generates the key for you.","title":"Deploy"},{"location":"console/deploy.html#deploy","text":"{: .no_toc } Connect the trusted code to be deployed in the Datacage .","title":"Deploy"},{"location":"console/deploy.html#set-up-your-own-algorithm","text":"Go to template Datavillage repository and fork it. If the branch to deploy isn't the default repository branch, you can set it in the Branch name field. Click Save to save your repository and the branch to deploy. Write your own algorithm using the provided utility python library to interact with the Datavillage services. This library allows your algorithm to access needed inforamtion such as: |:---------------| | Getting list of registered users | | Getting registered user data | | Getting application settings | | Saving algorithm result to user's pods | | Saving algorithm explains to user's pods | If you don't want to use the library, you can also make the http calls inside the algorithm. A OpenAPI documentation is available ths API section Enter the clone SSH git url of your repository. For GitHub you can find this URL here: Click the Generate deploy key , this will generate a public key and display it in the Deploy key text area. Copy this key and save it to your repository settings. For GitHub, you can follow these steps and skip the first 3 as Datavillage generates the key for you.","title":"Set up your own algorithm"},{"location":"console/header.html","text":"Header {: .no_toc } Keep track of information, status of your collaboration space. The header section contains multiple components (from left to right): Application name. Application internal ID. If enabled in settings: the Jupyter Console access link to interact and test the application algorithm in your notebook. The Consent Link from where you will be able to sign in your application with a user account. Status of the collaboration space Datacage .","title":"Header"},{"location":"console/header.html#header","text":"","title":"Header"},{"location":"console/header.html#no_toc","text":"Keep track of information, status of your collaboration space. The header section contains multiple components (from left to right): Application name. Application internal ID. If enabled in settings: the Jupyter Console access link to interact and test the application algorithm in your notebook. The Consent Link from where you will be able to sign in your application with a user account. Status of the collaboration space Datacage .","title":"{: .no_toc }"},{"location":"console/overview.html","text":"Overview {: .no_toc } Manage your Datacage and get an overview of the recent activities. Application actions You can take multiple actions on your application from there: Deploy / Redeploy your algorithm to the Datacage . Delete your application from the Datacage (only the algorithm will be deleted, you will be able to deploy later). Get Status of the Datacage Get Logs from the Datacage Application details Some application details (read only) Active users This is where you can see the number of users that have signed the consent through the passport web application accessible from Consent Link in the header section . Recent status history From there you can have a view on the 5 latest application status. By hovering the status, you can view some details of it.","title":"Overview"},{"location":"console/overview.html#overview","text":"","title":"Overview"},{"location":"console/overview.html#no_toc","text":"Manage your Datacage and get an overview of the recent activities.","title":"{: .no_toc }"},{"location":"console/overview.html#application-actions","text":"You can take multiple actions on your application from there: Deploy / Redeploy your algorithm to the Datacage . Delete your application from the Datacage (only the algorithm will be deleted, you will be able to deploy later). Get Status of the Datacage Get Logs from the Datacage","title":"Application actions"},{"location":"console/overview.html#application-details","text":"Some application details (read only)","title":"Application details"},{"location":"console/overview.html#active-users","text":"This is where you can see the number of users that have signed the consent through the passport web application accessible from Consent Link in the header section .","title":"Active users"},{"location":"console/overview.html#recent-status-history","text":"From there you can have a view on the 5 latest application status. By hovering the status, you can view some details of it.","title":"Recent status history"},{"location":"console/settings.html","text":"Overview {: .no_toc } Edit your collaboration space settings. Please note that the changes are NOT saved automatically, you need to save them manually to see them applied. Sandboxing By default, the Datacage isn't sandboxed, this means that the Datacage doesn't block any connection to any external services. When toggled on, you can still allow some connections by defining the allowed addresses. Enclaving When toggled on, the Datacage is deployed on TEE enclaves. Neo4j When toggled on, Neo4J database service is deployed. PostgreSQL When toggled on, PostgreSQL database service is deployed. Jupyter If enable, you can access your Jupyter notebook from the link in the header section . CRON triggers","title":"Settings"},{"location":"console/settings.html#overview","text":"","title":"Overview"},{"location":"console/settings.html#no_toc","text":"Edit your collaboration space settings. Please note that the changes are NOT saved automatically, you need to save them manually to see them applied.","title":"{: .no_toc }"},{"location":"console/settings.html#sandboxing","text":"By default, the Datacage isn't sandboxed, this means that the Datacage doesn't block any connection to any external services. When toggled on, you can still allow some connections by defining the allowed addresses.","title":"Sandboxing"},{"location":"console/settings.html#enclaving","text":"When toggled on, the Datacage is deployed on TEE enclaves.","title":"Enclaving"},{"location":"console/settings.html#neo4j","text":"When toggled on, Neo4J database service is deployed.","title":"Neo4j"},{"location":"console/settings.html#postgresql","text":"When toggled on, PostgreSQL database service is deployed.","title":"PostgreSQL"},{"location":"console/settings.html#jupyter","text":"If enable, you can access your Jupyter notebook from the link in the header section .","title":"Jupyter"},{"location":"console/settings.html#cron-triggers","text":"","title":"CRON triggers"}]}